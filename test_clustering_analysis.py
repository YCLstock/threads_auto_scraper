"""
èšé¡åˆ†æåŠŸèƒ½å–®å…ƒæ¸¬è©¦
æ¸¬è©¦ process_data.py ä¸­çš„ä¸»é¡Œèšé¡åˆ†æåŠŸèƒ½
"""

import pytest
import pandas as pd
import numpy as np
from datetime import datetime, timezone, timedelta
from unittest.mock import Mock, patch, MagicMock
import sys
import os

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from process_data import DataProcessor, TopicSummary

class TestClusteringAnalysis:
    """èšé¡åˆ†ææ¸¬è©¦é¡"""
    
    @pytest.fixture
    def processor(self):
        """å‰µå»ºæ¸¬è©¦ç”¨çš„æ•¸æ“šè™•ç†å™¨"""
        with patch('process_data.SupabaseManager'), \
             patch('process_data.jieba'), \
             patch('process_data.SentimentIntensityAnalyzer'):
            processor = DataProcessor()
            return processor
    
    @pytest.fixture
    def sample_clustering_df(self):
        """å‰µå»ºèšé¡åˆ†ææ¸¬è©¦æ•¸æ“š"""
        base_time = datetime.now(timezone.utc)
        
        # å‰µå»ºä¸åŒä¸»é¡Œçš„æ¸¬è©¦è²¼æ–‡
        data = []
        
        # ç§‘æŠ€ä¸»é¡Œè²¼æ–‡
        tech_posts = [
            'AIäººå·¥æ™ºæ…§æŠ€è¡“ç™¼å±•è¿…é€Ÿ',
            'æ©Ÿå™¨å­¸ç¿’ç®—æ³•å„ªåŒ–æ–¹æ³•',
            'æ·±åº¦å­¸ç¿’ç¥ç¶“ç¶²çµ¡æ‡‰ç”¨',
            'æ•¸æ“šç§‘å­¸åˆ†ææŠ€è¡“',
            'é›²ç«¯è¨ˆç®—å¹³å°æœå‹™'
        ]
        
        # è²¡ç¶“ä¸»é¡Œè²¼æ–‡
        finance_posts = [
            'è‚¡ç¥¨æŠ•è³‡ç­–ç•¥åˆ†æ',
            'åŠ å¯†è²¨å¹£å¸‚å ´è¶¨å‹¢',
            'æˆ¿åœ°ç”¢æŠ•è³‡æ©Ÿæœƒ',
            'åŸºé‡‘ç†è²¡è¦åŠƒå»ºè­°',
            'ç¶“æ¿ŸæŒ‡æ¨™å½±éŸ¿åˆ†æ'
        ]
        
        # ç”Ÿæ´»ä¸»é¡Œè²¼æ–‡
        life_posts = [
            'å¥åº·é£²é£Ÿç”Ÿæ´»ç¿’æ…£',
            'é‹å‹•å¥èº«è¨ˆåŠƒåˆ†äº«',
            'æ—…è¡Œæ”»ç•¥ç¶“é©—åˆ†äº«',
            'ç¾é£Ÿé¤å»³æ¨è–¦è©•åƒ¹',
            'é›»å½±éŸ³æ¨‚å¨›æ¨‚æ¨è–¦'
        ]
        
        all_posts = [
            (tech_posts, 'tech'),
            (finance_posts, 'finance'), 
            (life_posts, 'life')
        ]
        
        post_id = 0
        for posts_list, category in all_posts:
            for i, content in enumerate(posts_list):
                # ä¸åŒä¸»é¡Œæœ‰ä¸åŒçš„äº’å‹•æ°´å¹³
                if category == 'tech':
                    base_interactions = 150
                elif category == 'finance':
                    base_interactions = 100
                else:  # life
                    base_interactions = 80
                
                likes = base_interactions + np.random.randint(-30, 50)
                replies = max(1, likes // 5 + np.random.randint(-5, 10))
                reposts = max(0, likes // 8 + np.random.randint(-3, 8))
                
                data.append({
                    'post_id': f'post_{post_id}',
                    'username': f'user_{category}_{i}',
                    'content': content,
                    'timestamp': base_time - timedelta(hours=np.random.randint(1, 72)),
                    'likes': likes,
                    'replies': replies,
                    'reposts': reposts,
                    'total_interactions': likes + replies + reposts,
                    'heat_density': np.random.uniform(20, 90),
                    'freshness_score': np.random.uniform(0.3, 1.0)
                })
                post_id += 1
        
        return pd.DataFrame(data)
    
    def test_perform_topic_clustering_basic(self, processor, sample_clustering_df):
        """æ¸¬è©¦åŸºæœ¬ä¸»é¡Œèšé¡åŠŸèƒ½"""
        with patch('process_data.jieba.cut') as mock_cut, \
             patch.object(processor, '_analyze_cluster_sentiment') as mock_sentiment, \
             patch.object(processor, '_calculate_trending_score') as mock_trending:
            
            # Mock jieba åˆ†è©
            mock_cut.side_effect = lambda text: text.split()
            mock_sentiment.return_value = 'positive'
            mock_trending.return_value = 0.7
            
            topics = processor.perform_topic_clustering(sample_clustering_df)
            
            # æª¢æŸ¥è¿”å›é¡å‹å’ŒåŸºæœ¬çµæ§‹
            assert isinstance(topics, list)
            
            for topic in topics:
                assert isinstance(topic, TopicSummary)
                assert hasattr(topic, 'topic_id')
                assert hasattr(topic, 'topic_keywords')
                assert hasattr(topic, 'topic_name')
                assert hasattr(topic, 'post_count')
                assert hasattr(topic, 'average_heat_density')
                assert hasattr(topic, 'total_interactions')
                assert hasattr(topic, 'dominant_sentiment')
                assert hasattr(topic, 'trending_score')
                
                # æª¢æŸ¥æ•¸æ“šé¡å‹
                assert isinstance(topic.topic_id, int)
                assert isinstance(topic.topic_keywords, list)
                assert isinstance(topic.topic_name, str)
                assert isinstance(topic.post_count, int)
                assert isinstance(topic.average_heat_density, (int, float))
                assert isinstance(topic.total_interactions, int)
                assert isinstance(topic.dominant_sentiment, str)
                assert isinstance(topic.trending_score, (int, float))
                
                # æª¢æŸ¥æ•¸æ“šåˆç†æ€§
                assert topic.topic_id > 0
                assert len(topic.topic_keywords) > 0
                assert topic.post_count > 0
                assert topic.total_interactions >= 0
                assert 0 <= topic.trending_score <= 1
    
    def test_clustering_insufficient_data(self, processor):
        """æ¸¬è©¦æ•¸æ“šä¸è¶³æ™‚çš„èšé¡è¡Œç‚º"""
        # å‰µå»ºæ•¸æ“šé‡ä¸è¶³çš„æ¸¬è©¦æ•¸æ“š
        insufficient_data = pd.DataFrame({
            'post_id': ['post_1', 'post_2'],
            'content': ['çŸ­æ–‡æœ¬1', 'çŸ­æ–‡æœ¬2'],
            'total_interactions': [5, 3],  # ä½æ–¼é–¾å€¼
            'heat_density': [10, 8],
            'freshness_score': [0.5, 0.6]
        })
        
        topics = processor.perform_topic_clustering(insufficient_data)
        
        # æ•¸æ“šä¸è¶³æ™‚æ‡‰è©²è¿”å›ç©ºåˆ—è¡¨
        assert isinstance(topics, list)
        assert len(topics) == 0
    
    def test_clustering_empty_dataframe(self, processor):
        """æ¸¬è©¦ç©ºæ•¸æ“šæ¡†çš„èšé¡è™•ç†"""
        empty_df = pd.DataFrame()
        topics = processor.perform_topic_clustering(empty_df)
        
        assert isinstance(topics, list)
        assert len(topics) == 0
    
    def test_generate_topic_name(self, processor):
        """æ¸¬è©¦ä¸»é¡Œåç¨±ç”ŸæˆåŠŸèƒ½"""
        # æ¸¬è©¦ç§‘æŠ€é¡é—œéµè©
        tech_keywords = ['AI', 'äººå·¥æ™ºæ…§', 'æŠ€è¡“', 'æ•¸æ“š']
        tech_contents = ['AIæŠ€è¡“ç™¼å±•', 'æ•¸æ“šåˆ†ææ‡‰ç”¨']
        tech_name = processor._generate_topic_name(tech_keywords, tech_contents)
        
        assert isinstance(tech_name, str)
        assert len(tech_name) > 0
        assert 'AI' in tech_name or 'ç§‘æŠ€' in tech_name
        
        # æ¸¬è©¦è²¡ç¶“é¡é—œéµè©
        finance_keywords = ['æŠ•è³‡', 'è‚¡ç¥¨', 'é‡‘è', 'å¸‚å ´']
        finance_contents = ['æŠ•è³‡ç­–ç•¥', 'è‚¡ç¥¨åˆ†æ']
        finance_name = processor._generate_topic_name(finance_keywords, finance_contents)
        
        assert isinstance(finance_name, str)
        assert len(finance_name) > 0
        
        # æ¸¬è©¦ç©ºé—œéµè©
        empty_name = processor._generate_topic_name([], [])
        assert empty_name == "æœªçŸ¥ä¸»é¡Œ"
    
    def test_analyze_cluster_sentiment(self, processor):
        """æ¸¬è©¦èšé¡æƒ…æ„Ÿåˆ†æ"""
        # æ¸¬è©¦æ­£é¢å…§å®¹
        positive_contents = ['é€™å€‹ç”¢å“çœŸçš„å¾ˆæ£’ï¼', 'éå¸¸æ¨è–¦å¤§å®¶', 'æ•ˆæœè¶…ä¹é æœŸ']
        
        # Mock æƒ…æ„Ÿåˆ†æå™¨
        with patch.object(processor, 'sentiment_analyzer') as mock_analyzer:
            mock_analyzer.polarity_scores.side_effect = [
                {'compound': 0.8}, {'compound': 0.7}, {'compound': 0.9}
            ]
            
            sentiment = processor._analyze_cluster_sentiment(positive_contents)
            assert sentiment in ['positive', 'negative', 'neutral']
        
        # æ¸¬è©¦ç„¡æƒ…æ„Ÿåˆ†æå™¨çš„æƒ…æ³
        processor.sentiment_analyzer = None
        sentiment = processor._analyze_cluster_sentiment(positive_contents)
        assert sentiment == 'neutral'
        
        # æ¸¬è©¦ç©ºå…§å®¹
        sentiment = processor._analyze_cluster_sentiment([])
        assert sentiment == 'neutral'
    
    def test_calculate_trending_score(self, processor):
        """æ¸¬è©¦è¶¨å‹¢åˆ†æ•¸è¨ˆç®—"""
        current_time = datetime.now(timezone.utc)
        
        # å‰µå»ºæ¸¬è©¦èšé¡æ•¸æ“š
        cluster_data = pd.DataFrame({
            'timestamp': [
                current_time - timedelta(hours=6),
                current_time - timedelta(hours=3),
                current_time - timedelta(hours=1)
            ],
            'total_interactions': [100, 150, 200],
            'freshness_score': [0.7, 0.8, 0.9]
        })
        
        score = processor._calculate_trending_score(cluster_data)
        
        assert isinstance(score, (int, float))
        assert 0 <= score <= 1.0
        
        # æ¸¬è©¦å–®æ¢è¨˜éŒ„
        single_record = pd.DataFrame({
            'timestamp': [current_time],
            'total_interactions': [100],
            'freshness_score': [0.8]
        })
        
        single_score = processor._calculate_trending_score(single_record)
        assert single_score == 0.0
        
        # æ¸¬è©¦ç©ºæ•¸æ“šæ¡†
        empty_df = pd.DataFrame(columns=['timestamp', 'total_interactions', 'freshness_score'])
        empty_score = processor._calculate_trending_score(empty_df)
        assert empty_score == 0.0
    
    def test_clustering_with_different_thresholds(self, processor, sample_clustering_df):
        """æ¸¬è©¦ä¸åŒé–¾å€¼ä¸‹çš„èšé¡çµæœ"""
        original_threshold = processor.min_interactions_threshold
        
        try:
            # æ¸¬è©¦é«˜é–¾å€¼
            processor.min_interactions_threshold = 1000  # å¾ˆé«˜çš„é–¾å€¼
            with patch('process_data.jieba.cut') as mock_cut:
                mock_cut.side_effect = lambda text: text.split()
                topics_high = processor.perform_topic_clustering(sample_clustering_df)
                
            # é«˜é–¾å€¼ä¸‹æ‡‰è©²æ²’æœ‰æˆ–å¾ˆå°‘èšé¡çµæœ
            assert isinstance(topics_high, list)
            
            # æ¸¬è©¦ä½é–¾å€¼
            processor.min_interactions_threshold = 1  # å¾ˆä½çš„é–¾å€¼
            with patch('process_data.jieba.cut') as mock_cut, \
                 patch.object(processor, '_analyze_cluster_sentiment') as mock_sentiment, \
                 patch.object(processor, '_calculate_trending_score') as mock_trending:
                
                mock_cut.side_effect = lambda text: text.split()
                mock_sentiment.return_value = 'positive'
                mock_trending.return_value = 0.5
                
                topics_low = processor.perform_topic_clustering(sample_clustering_df)
            
            # ä½é–¾å€¼ä¸‹æ‡‰è©²æœ‰æ›´å¤šèšé¡çµæœ
            assert isinstance(topics_low, list)
            
        finally:
            # æ¢å¾©åŸå§‹é–¾å€¼
            processor.min_interactions_threshold = original_threshold
    
    def test_clustering_with_different_cluster_numbers(self, processor, sample_clustering_df):
        """æ¸¬è©¦ä¸åŒèšé¡æ•¸é‡çš„å½±éŸ¿"""
        original_max_topics = processor.max_topics
        
        try:
            # æ¸¬è©¦è¼ƒå°‘çš„èšé¡æ•¸
            processor.max_topics = 2
            with patch('process_data.jieba.cut') as mock_cut, \
                 patch.object(processor, '_analyze_cluster_sentiment') as mock_sentiment, \
                 patch.object(processor, '_calculate_trending_score') as mock_trending:
                
                mock_cut.side_effect = lambda text: text.split()
                mock_sentiment.return_value = 'neutral'
                mock_trending.return_value = 0.6
                
                topics = processor.perform_topic_clustering(sample_clustering_df)
            
            assert isinstance(topics, list)
            if len(topics) > 0:
                assert len(topics) <= 2
            
        finally:
            # æ¢å¾©åŸå§‹è¨­ç½®
            processor.max_topics = original_max_topics
    
    def test_clustering_error_handling(self, processor, sample_clustering_df):
        """æ¸¬è©¦èšé¡éç¨‹ä¸­çš„éŒ¯èª¤è™•ç†"""
        with patch('process_data.jieba.cut') as mock_cut:
            # æ¨¡æ“¬ jieba åˆ†è©éŒ¯èª¤
            mock_cut.side_effect = Exception("åˆ†è©éŒ¯èª¤")
            
            topics = processor.perform_topic_clustering(sample_clustering_df)
            
            # å³ä½¿å‡ºéŒ¯ä¹Ÿæ‡‰è©²è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸æ‡‰è©²æ‹‹å‡ºç•°å¸¸
            assert isinstance(topics, list)
    
    def test_clustering_with_special_characters(self, processor):
        """æ¸¬è©¦åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡æœ¬èšé¡"""
        special_data = pd.DataFrame({
            'post_id': [f'post_{i}' for i in range(5)],
            'username': [f'user_{i}' for i in range(5)],
            'content': [
                'é€™æ˜¯åŒ…å«emojiçš„æ–‡æœ¬ğŸ˜ŠğŸ‘',
                'Text with English and ä¸­æ–‡æ··åˆ',
                '@ç”¨æˆ¶å #æ¨™ç±¤ https://example.com',
                'æ•¸å­—123å’Œç¬¦è™Ÿ!@#$%^&*()',
                '   ç©ºç™½   å’Œ   é–“è·   '
            ],
            'total_interactions': [100, 120, 80, 90, 110],
            'heat_density': [50, 60, 40, 45, 55],
            'freshness_score': [0.8, 0.7, 0.9, 0.6, 0.8]
        })
        
        with patch('process_data.jieba.cut') as mock_cut, \
             patch.object(processor, '_analyze_cluster_sentiment') as mock_sentiment, \
             patch.object(processor, '_calculate_trending_score') as mock_trending:
            
            mock_cut.side_effect = lambda text: ['æ–‡æœ¬', 'å…§å®¹', 'æ¸¬è©¦']
            mock_sentiment.return_value = 'neutral'
            mock_trending.return_value = 0.5
            
            topics = processor.perform_topic_clustering(special_data)
            
            # æ‡‰è©²èƒ½æ­£å¸¸è™•ç†ç‰¹æ®Šå­—ç¬¦
            assert isinstance(topics, list)
    
    def test_clustering_keyword_extraction_integration(self, processor, sample_clustering_df):
        """æ¸¬è©¦èšé¡èˆ‡é—œéµè©æå–çš„é›†æˆ"""
        with patch('process_data.jieba.cut') as mock_cut, \
             patch('process_data.TfidfVectorizer') as mock_vectorizer, \
             patch.object(processor, '_analyze_cluster_sentiment') as mock_sentiment, \
             patch.object(processor, '_calculate_trending_score') as mock_trending:
            
            # Mock TF-IDF å‘é‡åŒ–å™¨
            mock_tfidf_instance = MagicMock()
            mock_vectorizer.return_value = mock_tfidf_instance
            
            # æ¨¡æ“¬å‘é‡åŒ–çµæœ
            mock_tfidf_matrix = MagicMock()
            mock_tfidf_instance.fit_transform.return_value = mock_tfidf_matrix
            mock_tfidf_instance.get_feature_names_out.return_value = ['AI', 'æŠ€è¡“', 'æŠ•è³‡', 'å¸‚å ´', 'ç”Ÿæ´»']
            
            # Mock K-means èšé¡
            with patch('process_data.KMeans') as mock_kmeans:
                mock_kmeans_instance = MagicMock()
                mock_kmeans.return_value = mock_kmeans_instance
                
                # æ¨¡æ“¬èšé¡çµæœ
                mock_kmeans_instance.fit_predict.return_value = np.array([0, 0, 1, 1, 2])
                mock_kmeans_instance.cluster_centers_ = np.array([
                    [0.8, 0.6, 0.1, 0.1, 0.1],  # ç§‘æŠ€èšé¡ä¸­å¿ƒ
                    [0.1, 0.1, 0.8, 0.6, 0.1],  # è²¡ç¶“èšé¡ä¸­å¿ƒ
                    [0.1, 0.1, 0.1, 0.1, 0.8]   # ç”Ÿæ´»èšé¡ä¸­å¿ƒ
                ])
                
                mock_cut.side_effect = lambda text: text.split()
                mock_sentiment.return_value = 'positive'
                mock_trending.return_value = 0.7
                
                topics = processor.perform_topic_clustering(sample_clustering_df)
                
                assert isinstance(topics, list)
                if len(topics) > 0:
                    for topic in topics:
                        assert len(topic.topic_keywords) > 0
                        assert all(isinstance(kw, str) for kw in topic.topic_keywords)

# æ€§èƒ½æ¸¬è©¦é¡
class TestClusteringPerformance:
    """èšé¡åˆ†ææ€§èƒ½æ¸¬è©¦"""
    
    @pytest.fixture
    def processor(self):
        """å‰µå»ºæ¸¬è©¦ç”¨çš„æ•¸æ“šè™•ç†å™¨"""
        with patch('process_data.SupabaseManager'), \
             patch('process_data.jieba'), \
             patch('process_data.SentimentIntensityAnalyzer'):
            return DataProcessor()
    
    def test_clustering_performance_large_dataset(self, processor):
        """æ¸¬è©¦å¤§æ•¸æ“šé›†çš„èšé¡æ€§èƒ½"""
        # å‰µå»ºè¼ƒå¤§çš„æ¸¬è©¦æ•¸æ“šé›†
        n_posts = 1000
        current_time = datetime.now(timezone.utc)
        
        large_data = pd.DataFrame({
            'post_id': [f'post_{i}' for i in range(n_posts)],
            'username': [f'user_{i % 100}' for i in range(n_posts)],
            'content': [f'æ¸¬è©¦å…§å®¹ {i % 10}' for i in range(n_posts)],
            'timestamp': [current_time - timedelta(hours=i % 72) for i in range(n_posts)],
            'total_interactions': np.random.randint(50, 500, n_posts),
            'heat_density': np.random.uniform(20, 90, n_posts),
            'freshness_score': np.random.uniform(0.3, 1.0, n_posts)
        })
        
        with patch('process_data.jieba.cut') as mock_cut, \
             patch.object(processor, '_analyze_cluster_sentiment') as mock_sentiment, \
             patch.object(processor, '_calculate_trending_score') as mock_trending:
            
            mock_cut.side_effect = lambda text: text.split()
            mock_sentiment.return_value = 'neutral'
            mock_trending.return_value = 0.5
            
            import time
            start_time = time.time()
            topics = processor.perform_topic_clustering(large_data)
            end_time = time.time()
            
            # æª¢æŸ¥è™•ç†æ™‚é–“ï¼ˆæ‡‰è©²åœ¨åˆç†ç¯„åœå…§ï¼‰
            processing_time = end_time - start_time
            assert processing_time < 30.0  # 30ç§’å…§å®Œæˆ
            
            # æª¢æŸ¥çµæœ
            assert isinstance(topics, list)

if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    pytest.main([__file__, "-v", "--tb=short"])